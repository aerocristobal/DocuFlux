{
  "permissions": {
    "allow": [
      "WebFetch(domain:github.com)",
      "WebFetch(domain:hub.docker.com)",
      "WebFetch(domain:raw.githubusercontent.com)",
      "WebSearch",
      "Bash(git submodule:*)",
      "Bash(git rm:*)",
      "Bash(pytest:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(docker-compose down:*)",
      "Bash(docker-compose up:*)",
      "Bash(docker-compose ps:*)",
      "Bash(curl:*)",
      "Bash(docker-compose logs:*)",
      "Bash(docker exec:*)",
      "Bash(docker-compose restart:*)",
      "Bash(docker logs:*)",
      "Bash(docker restart:*)",
      "Bash(docker inspect:*)",
      "Bash(docker-compose stop:*)",
      "Bash(docker-compose rm:*)",
      "Bash(docker-compose pull:*)",
      "Bash(docker-compose build:*)",
      "Bash(docker-compose exec:*)",
      "Bash(du:*)",
      "Bash(grep:*)",
      "Bash(python3:*)",
      "Bash(chmod:*)",
      "Bash(gh api:*)",
      "Bash(gh auth status:*)",
      "Bash(gh auth login:*)",
      "Bash(tee:*)",
      "Bash(docker-compose:*)",
      "Bash(ls:*)",
      "Bash(ps:*)",
      "Bash(docker ps:*)",
      "Bash(docker rmi:*)",
      "Bash(while ps -p 168268)",
      "Bash(do sleep 60)",
      "Bash(done)",
      "Bash(while ps aux)",
      "Bash(do sleep 30)",
      "Bash(do sleep 15)",
      "Bash(git -c core.fileMode=false add:*)",
      "Bash(docker system df:*)",
      "Bash(pandoc:*)",
      "Bash(docker cp:*)",
      "Bash(gh issue view:*)",
      "Bash(docker build:*)",
      "Bash(pkill:*)",
      "Bash(gh issue list:*)",
      "Bash(find:*)",
      "Bash(gh label:*)",
      "Bash(/tmp/epic-30-2.md <<'EOF'\n## Story 30.2: Optimize Job Listing Performance\n\n**Priority**: P0-Critical | **Effort**: 6 hours | **Epic**: 30\n\n**As a** user  \n**I want** job listing to load quickly  \n**So that** I can see my conversion history without delays\n\n### Current Problem\n- Job listing API performs O\\(n\\) filesystem walks\n- 500ms latency for 100 jobs\n- Each job requires multiple Redis calls and os.stat\\(\\) checks\n\n### Technical Implementation\n- Cache job metadata in Redis `job_list` sorted set \\(zadd with timestamp\\)\n- Return cached list on `/api/jobs` \\(single Redis call\\)\n- Update cache on job creation/completion\n- Implement pagination \\(limit 50 per page\\)\n\n### Files to Modify\n- `web/app.py` - Optimize `/api/jobs` endpoint \\(~50 lines modified\\)\n- `web/app.py` - Add cache update in job creation \\(~10 lines\\)\n- `web/app.py` - Add cache invalidation on job status change \\(~15 lines\\)\n\n### Verification\n```bash\n# Benchmark before\ntime curl http://localhost:5000/api/jobs\n# Expect: ~500ms for 100 jobs\n\n# After optimization\ntime curl http://localhost:5000/api/jobs\n# Target: <50ms for 100 jobs \\(10x improvement\\)\n```\n\n### Definition of Done\n- [ ] Job listing latency <50ms for 100 jobs\n- [ ] Single Redis call per request \\(measured with MONITOR\\)\n- [ ] Pagination implemented \\(50 jobs per page\\)\n- [ ] Cache invalidation verified \\(job status changes reflected\\)\n- [ ] All tests pass\n- [ ] Load test with 1000 jobs completes <200ms\n\n**References**: Plan lines 107-185\nEOF)",
      "Bash(gh issue create:*)",
      "Bash(/tmp/epic-30-3.md <<'EOF'\n## Story 30.3: Eliminate Redundant Redis Calls\n\n**Priority**: P0-Critical | **Effort**: 6 hours | **Epic**: 30\n\n**As a** system operator  \n**I want** Redis calls minimized during conversions  \n**So that** system throughput increases and Redis load decreases\n\n### Current Problem\n- 15-20 Redis calls per conversion\n- Redundant metadata fetches\n- No batching of updates\n\n### Technical Implementation\n- Batch Redis operations using pipelines\n- Cache metadata locally during conversion\n- Use HMSET for bulk updates\n- Target: <5 Redis calls per conversion\n\n### Files to Modify\n- `worker/tasks.py` - Add Redis pipeline batching \\(~40 lines\\)\n- `web/app.py` - Batch status checks \\(~30 lines\\)\n- `shared/redis_utils.py` - Create batching helpers \\(new, ~60 lines\\)\n\n### Verification\n```bash\n# Monitor Redis during conversion\nredis-cli MONITOR > /tmp/redis.log &\n# Upload and convert file\ncurl -X POST http://localhost:5000/convert -F \"file=@test.pdf\" -F \"from_format=pdf\" -F \"to_format=markdown\"\n# Count commands\ngrep -c \"HSET\\\\|HGETALL\\\\|HMSET\" /tmp/redis.log\n# Target: <5 calls\n```\n\n### Definition of Done\n- [ ] Redis calls reduced from 15-20 to <5 per conversion\n- [ ] All metadata updates batched\n- [ ] No functionality regressions\n- [ ] 70% reduction in Redis load verified with MONITOR\n- [ ] All tests pass\n\n**References**: Plan lines 188-262\nEOF)",
      "Bash(/tmp/epic-30-4.md <<'EOF'\n## Story 30.4: Replace Blocking I/O in Conversion Monitoring\n\n**Priority**: P0-Critical | **Effort**: 4 hours | **Epic**: 30\n\n**As a** system  \n**I want** non-blocking I/O for conversion monitoring  \n**So that** the web service can handle more concurrent requests\n\n### Current Problem\n- Conversion status checks use blocking Redis calls\n- Thread pool exhaustion under high load\n- No async/await patterns\n\n### Technical Implementation\n- Use Redis pub/sub for status updates\n- Workers publish status changes to channel\n- Web subscribes to channel for real-time updates\n- Remove polling loops\n\n### Files to Modify\n- `worker/tasks.py` - Add pub/sub publishing \\(~20 lines\\)\n- `web/app.py` - Add pub/sub subscription \\(~30 lines\\)\n- `web/app.py` - Remove polling loops \\(~15 lines removed\\)\n\n### Verification\n```bash\n# Load test\nlocust -f tests/load/locustfile.py --users 100 --spawn-rate 10\n# Before: 10 req/s, thread exhaustion\n# After: 50 req/s, no thread exhaustion\n```\n\n### Definition of Done\n- [ ] Redis pub/sub implemented for status updates\n- [ ] No blocking Redis calls in request handlers\n- [ ] Concurrent request capacity: 50 req/s \\(5x improvement\\)\n- [ ] No thread pool exhaustion under load\n- [ ] All tests pass\n\n**References**: Plan lines 265-341\nEOF)",
      "Bash(/tmp/epic-31-1.md <<'EOF'\n## Story 31.1: Fix Broken Worker Tests \\(Issue #12\\)\n\n**Priority**: P0-Critical | **Effort**: 8 hours | **Epic**: 31\n\n**As a** developer  \n**I want** worker tests to match the actual PdfConverter API  \n**So that** tests validate real behavior instead of mocked stubs\n\n### Current Problem\nTests mock `subprocess.run` but code uses `PdfConverter` class:\n\n```python\n# WRONG: tests/unit/test_worker.py\n@patch\\('tasks.subprocess.run'\\)\ndef test_convert_with_marker\\(mock_run\\):\n    mock_run.return_value = MagicMock\\(returncode=0\\)\n    # But actual code: converter = PdfConverter\\(...\\)\n```\n\n### Technical Implementation\nReplace with correct mocking:\n\n```python\n# RIGHT: Mock the actual API\n@patch\\('tasks.PdfConverter'\\)\ndef test_convert_with_marker\\(mock_converter_class\\):\n    mock_instance = MagicMock\\(\\)\n    mock_instance.return_value = \\(\"# Markdown\", [], {\"pages\": 5}\\)\n    mock_converter_class.return_value = mock_instance\n```\n\n### Files to Modify\n- `tests/unit/test_worker.py` - Replace subprocess mocks \\(~80 lines modified\\)\n- `tests/conftest.py` - Add PdfConverter fixtures \\(~40 lines\\)\n- `tests/unit/test_worker.py` - Add lazy loading tests \\(~30 lines\\)\n- `tests/unit/test_worker.py` - Add GPU detection tests \\(~25 lines\\)\n\n### Verification\n```bash\npytest tests/unit/test_worker.py -v --cov=worker\n# Target: >90% coverage for convert_with_marker\\(\\)\n```\n\n### Definition of Done\n- [ ] All worker tests use PdfConverter mocks \\(not subprocess\\)\n- [ ] Test coverage for convert_with_marker\\(\\) reaches 90%\n- [ ] Lazy loading behavior tested\n- [ ] GPU/CPU detection logic tested\n- [ ] All tests pass: `pytest tests/unit/test_worker.py -v`\n- [ ] Issue #12 closed with updated test patterns documented\n\n**References**: Plan lines 360-438\nEOF)",
      "Bash(/tmp/epic-31-2.md <<'EOF'\n## Story 31.2: Add End-to-End Integration Tests\n\n**Priority**: P0-Critical | **Effort**: 6 hours | **Epic**: 31\n\n**As a** developer  \n**I want** full integration tests  \n**So that** I can verify the entire upload→convert→download pipeline\n\n### Technical Implementation\n- Create `tests/integration/` directory\n- Test full conversion pipeline \\(Pandoc and Marker\\)\n- Test multi-file output handling\n- Test cleanup/retention policies\n- Test WebSocket updates\n\n### Files to Create\n- `tests/integration/test_conversion_pipeline.py` \\(~120 lines\\)\n- `tests/integration/test_multi_file_output.py` \\(~80 lines\\)\n- `tests/integration/test_cleanup.py` \\(~60 lines\\)\n- `tests/fixtures/sample_files/` - Add test files\n\n### Test Scenarios\n```python\ndef test_full_pdf_to_markdown_conversion\\(\\):\n    \"\"\"Upload PDF → Convert with Marker → Download markdown + images ZIP\"\"\"\n    \ndef test_pandoc_docx_conversion\\(\\):\n    \"\"\"Upload DOCX → Convert with Pandoc → Download markdown\"\"\"\n    \ndef test_cleanup_after_retention\\(\\):\n    \"\"\"Upload → Convert → Wait 11 minutes → Verify cleanup\"\"\"\n```\n\n### Verification\n```bash\npytest tests/integration/ -v\n# All integration tests pass\n# Coverage for full pipeline: 100%\n```\n\n### Definition of Done\n- [ ] Integration tests directory created\n- [ ] Full conversion pipeline tested \\(5+ scenarios\\)\n- [ ] Multi-file output tested\n- [ ] Cleanup/retention tested\n- [ ] WebSocket updates tested\n- [ ] All tests pass with real Redis and file operations\n\n**References**: Plan lines 441-504\nEOF)",
      "Bash(/tmp/epic-31-3.md <<'EOF'\n## Story 31.3: Establish Coverage Baseline and CI Integration\n\n**Priority**: P0-Critical | **Effort**: 2 hours | **Epic**: 31\n\n**As a** project maintainer  \n**I want** automated coverage enforcement  \n**So that** code quality doesn't regress\n\n### Technical Implementation\n- Set coverage target: 80% overall\n- Configure pytest-cov in CI\n- Add coverage badge to README\n- Fail CI if coverage drops below threshold\n\n### Files to Modify\n- `.github/workflows/test.yml` - Add coverage enforcement \\(~20 lines\\)\n- `pytest.ini` - Configure coverage settings \\(~10 lines\\)\n- `README.md` - Add coverage badge \\(~5 lines\\)\n\n### Verification\n```bash\n# Local coverage check\npytest tests/ --cov=web --cov=worker --cov=shared --cov-report=html --cov-fail-under=80\n\n# CI should enforce on PR\n# Push PR with low coverage → CI fails\n```\n\n### Definition of Done\n- [ ] Coverage target set to 80%\n- [ ] CI fails if coverage <80%\n- [ ] Coverage badge added to README\n- [ ] Coverage report generated in CI artifacts\n- [ ] All tests pass with coverage enforcement\n\n**References**: Plan lines 507-563\nEOF)",
      "Bash(/tmp/epic-32-1.md <<'EOF'\n## Story 32.1: Eliminate Code Duplication - 2,351 Lines\n\n**Priority**: P1-High | **Effort**: 12 hours | **Epic**: 32\n\n**As a** developer  \n**I want** duplicated code consolidated into shared modules  \n**So that** bugs are fixed in one place, not eight\n\n### Duplicates Identified\n1. `encryption.py` - web/ and worker/ \\(355 × 2 = 710 lines\\)\n2. `key_manager.py` - web/ and worker/ \\(389 × 2 = 778 lines\\)\n3. `redis_encryption.py` - web/ and worker/ \\(264 × 2 = 528 lines\\)\n4. `secrets_manager.py` - web/ and worker/ \\(268 × 2 = 536 lines\\)\n\n**Total**: 2,351 lines of duplicate code \\(34% of codebase\\)\n\n### Technical Implementation\n- Create `shared/__init__.py`\n- Create `shared/redis_utils.py` \\(~80 lines consolidated\\)\n- Create `shared/validation.py` \\(~60 lines\\)\n- Create `shared/path_utils.py` \\(~50 lines\\)\n- Create `shared/system_utils.py` \\(~40 lines\\)\n- Move security modules to `shared/security/`\n\n### Verification\n```bash\n# Check dedup\ncloc web/ worker/ shared/ --by-file | grep -E \"\\(encryption|key_manager|redis_encryption|secrets_manager\\)\"\n# Before: ~2,351 lines\n# After: ~500 lines in shared/ \\(referenced by web/ and worker/\\)\n\npytest tests/ -v\n# All tests still pass\n```\n\n### Definition of Done\n- [ ] `shared/` module created with consolidated code\n- [ ] 2,351 lines reduced to <500 \\(in shared/\\)\n- [ ] All tests pass\n- [ ] No functionality changes\n- [ ] Imports updated across all files\n- [ ] Coverage maintained ≥80%\n\n**References**: Plan lines 581-656\nEOF)",
      "Bash(/tmp/epic-33-1.md <<'EOF'\n## Story 33.1: Enable Redis TLS \\(Issue #8\\)\n\n**Priority**: P1-High | **Effort**: 6 hours | **Epic**: 33\n\n**As a** security engineer  \n**I want** Redis communication encrypted with TLS  \n**So that** credentials and job metadata are protected in transit\n\n### Current Issues\n1. Certificates not generated\n2. Celery serializer mismatch \\(web uses JSON, worker uses pickle\\)\n\n### Technical Implementation\n1. Generate certificates with `./scripts/generate-redis-certs.sh`\n2. Update docker-compose.yml Redis service:\n   ```yaml\n   command: redis-server --tls-port 6380 --port 0 --tls-cert-file /certs/redis.crt --tls-key-file /certs/redis.key --tls-ca-cert-file /certs/ca.crt\n   ```\n3. Update connection URLs to `rediss://redis:6380`\n4. Fix Celery serializer on both web and worker \\(`json` on both\\)\n\n### Files to Modify\n- `docker-compose.yml` - Enable Redis TLS \\(+8 lines\\)\n- `web/app.py` - Update to rediss:// URLs\n- `worker/tasks.py` - Update to rediss:// URLs, fix serializer\n- `web/app.py` - Fix Celery serializer config\n\n### Verification\n```bash\n# Generate certificates\n./scripts/generate-redis-certs.sh\n\n# Verify TLS\ndocker exec docuflux-redis redis-cli -h redis -p 6380 --tls --cacert /certs/ca.crt ping\n# Should return: PONG\n\n# Verify full conversion works\ncurl -X POST http://localhost:5000/convert -F \"file=@test.pdf\" -F \"from_format=pdf\" -F \"to_format=markdown\"\n```\n\n### Definition of Done\n- [ ] Certificates generated and mounted\n- [ ] Redis TLS enabled and enforced \\(no plaintext port\\)\n- [ ] Web and worker connect via TLS\n- [ ] Celery serializer mismatch resolved \\(JSON both sides\\)\n- [ ] Integration tests pass with TLS\n- [ ] Documentation updated\n- [ ] Issue #8 closed\n\n**References**: Plan lines 994-1059\nEOF)",
      "Bash(/tmp/epic-32-2.md <<'EOF'\n## Story 32.2: Refactor Complex Functions\n\n**Priority**: P1-High | **Effort**: 10 hours | **Epic**: 32\n\n**As a** developer  \n**I want** complex functions split into smaller units  \n**So that** code is easier to test and maintain\n\n### Complex Functions Identified\n- `convert_with_marker\\(\\)` in tasks.py - 130+ lines, cyclomatic complexity >15\n- `cleanup_old_files\\(\\)` in tasks.py - 130+ lines\n- `/convert` route in app.py - 120+ lines\n- `index\\(\\)` route in app.py - 100+ lines\n\n### Technical Implementation\n- Break each function into 3-4 focused helpers \\(<50 lines each\\)\n- Extract shared logic into `shared/` module\n- Maintain backward compatibility\n\n### Target Structure\n```python\n# Before: convert_with_marker\\(\\) 130 lines\n# After:\ndef _prepare_marker_options\\(request_options\\): ...  # ~20 lines\ndef _run_marker_conversion\\(options\\): ...  # ~30 lines\ndef _handle_marker_output\\(result, job_id\\): ...  # ~25 lines\ndef _cleanup_marker_resources\\(converter\\): ...  # ~15 lines\ndef convert_with_marker\\(job_id, ...\\): ...  # ~20 lines \\(orchestrates above\\)\n```\n\n### Verification\n```bash\nradon cc web/app.py worker/tasks.py -a\n# Before: avg complexity >15\n# After: avg complexity <8\n\npytest tests/ -v\n# All tests pass\n```\n\n### Definition of Done\n- [ ] All 4 complex functions refactored \\(<50 lines each\\)\n- [ ] Average cyclomatic complexity <8\n- [ ] All tests pass\n- [ ] Code coverage maintained\n- [ ] No behavior changes\n\n**References**: Plan lines 659-734\nEOF)",
      "Bash(/tmp/epic-32-3.md <<'EOF'\n## Story 32.3: Centralize Configuration Management\n\n**Priority**: P1-High | **Effort**: 8 hours | **Epic**: 32\n\n**As a** developer  \n**I want** all configuration in one place  \n**So that** magic numbers and scattered settings are easy to find and change\n\n### Current Problems\n- 20+ magic numbers scattered across files\n- Retention times defined in multiple places\n- File size limits inconsistently defined\n- No central config module\n\n### Technical Implementation\n- Create `shared/config.py` with all constants\n- Use environment variables with defaults\n- Document each setting\n\n### Example config.py\n```python\n# shared/config.py\nclass Config:\n    # Retention policy\n    RETENTION_SUCCESS_DOWNLOADED_MINS = int\\(os.getenv\\(\"RETENTION_SUCCESS_DOWNLOADED\", 10\\)\\)\n    RETENTION_SUCCESS_PENDING_HOURS = int\\(os.getenv\\(\"RETENTION_SUCCESS_PENDING\", 1\\)\\)\n    RETENTION_FAILURE_MINS = int\\(os.getenv\\(\"RETENTION_FAILURE\", 5\\)\\)\n    \n    # File limits\n    MAX_UPLOAD_SIZE_MB = int\\(os.getenv\\(\"MAX_UPLOAD_SIZE_MB\", 100\\)\\)\n    LARGE_FILE_THRESHOLD_MB = int\\(os.getenv\\(\"LARGE_FILE_THRESHOLD_MB\", 5\\)\\)\n    \n    # Redis keys\n    JOB_KEY_PREFIX = \"job:\"\n    GPU_STATUS_KEY = \"marker:gpu_status\"\n```\n\n### Files to Create/Modify\n- `shared/config.py` \\(new, ~80 lines\\)\n- `web/app.py` - Replace magic numbers with Config constants\n- `worker/tasks.py` - Replace magic numbers with Config constants\n\n### Definition of Done\n- [ ] `shared/config.py` created with all constants\n- [ ] 20+ magic numbers replaced\n- [ ] All retention/limit values centralized\n- [ ] Environment variable overrides documented\n- [ ] All tests pass\n\n**References**: Plan lines 737-824\nEOF)",
      "Bash(/tmp/epic-32-4.md <<'EOF'\n## Story 32.4: Add Comprehensive Docstrings\n\n**Priority**: P1-High | **Effort**: 6 hours | **Epic**: 32\n\n**As a** developer  \n**I want** all public functions documented  \n**So that** new contributors can quickly understand the codebase\n\n### Technical Implementation\n- Add docstrings to all functions in web/app.py and worker/tasks.py\n- Document parameters, return values, and side effects\n- Use Google docstring format\n- Target: 95% docstring coverage\n\n### Example\n```python\ndef convert_document\\(job_id: str, from_format: str, to_format: str\\) -> None:\n    \"\"\"Convert a document using Pandoc.\n    \n    Args:\n        job_id: UUID identifying this conversion job\n        from_format: Source format \\(e.g., 'pdf', 'docx'\\)\n        to_format: Target format \\(e.g., 'markdown', 'html'\\)\n        \n    Side Effects:\n        - Updates Redis job metadata with progress\n        - Writes output to data/outputs/{job_id}/\n        - Emits WebSocket events to connected clients\n        \n    Raises:\n        ConversionError: If Pandoc fails or output not generated\n    \"\"\"\n```\n\n### Verification\n```bash\ninterrogate -vv web/ worker/ shared/\n# Target: ≥95% coverage\n```\n\n### Definition of Done\n- [ ] 95% docstring coverage \\(web/app.py, worker/tasks.py, shared/\\)\n- [ ] All public functions documented\n- [ ] Parameters and return types documented\n- [ ] Side effects documented\n- [ ] interrogate check passes\n\n**References**: Plan lines 827-906\nEOF)",
      "Bash(/tmp/epic-32-5.md <<'EOF'\n## Story 32.5: Remove Dead Code and Unused Imports\n\n**Priority**: P1-High | **Effort**: 4 hours | **Epic**: 32\n\n**As a** developer  \n**I want** dead code removed  \n**So that** the codebase is clean and navigable\n\n### Technical Implementation\n- Identify unused imports with autoflake\n- Find dead code with vulture\n- Remove or comment with explanation\n- No deletions without understanding purpose\n\n### Verification\n```bash\n# Find unused imports\nautoflake --check --remove-all-unused-imports -r web/ worker/\n\n# Find dead code\nvulture web/ worker/ --min-confidence 80\n\n# After cleanup\nautoflake --check -r web/ worker/\n# Should show no unused imports\n```\n\n### Definition of Done\n- [ ] All unused imports removed\n- [ ] Dead code removed or documented\n- [ ] No functionality removed\n- [ ] All tests pass\n- [ ] autoflake check passes with 0 issues\n\n**References**: Plan lines 909-975\nEOF)",
      "Bash(/tmp/epic-33-2.md <<'EOF'\n## Story 33.2: Remove Exposed Redis Port\n\n**Priority**: P1-High | **Effort**: 4 hours | **Epic**: 33\n\n**As a** security engineer  \n**I want** Redis accessible only within Docker network  \n**So that** the database is not exposed to the internet\n\n### Current State\n- Redis port 6379 exposed on 0.0.0.0 in docker-compose.yml\n- Anyone on the network can connect to Redis\n- No authentication required for local connections\n\n### Technical Implementation\n- Remove `ports: [\"6379:6379\"]` from Redis service in docker-compose.yml\n- Move web and worker to same Docker network\n- Enable Redis AUTH password\n- Verify internal connectivity works\n\n### Files to Modify\n- `docker-compose.yml` - Remove Redis port exposure, add network isolation\n- `web/app.py` - Add Redis AUTH if needed\n- `worker/tasks.py` - Add Redis AUTH if needed\n\n### Verification\n```bash\n# After fix\nnmap -p 6379 localhost\n# Should show: port closed\n\n# Internal connectivity\ndocker exec docuflux-web redis-cli -h redis ping\n# Should return: PONG\n```\n\n### Definition of Done\n- [ ] Redis port 6379 NOT exposed externally\n- [ ] Docker network isolation configured\n- [ ] Internal services still communicate\n- [ ] Redis AUTH optionally enabled\n- [ ] All integration tests pass\n\n**References**: Plan lines 1062-1130\nEOF)",
      "Bash(/tmp/epic-33-4.md <<'EOF'\n## Story 33.4: Implement API Key Authentication\n\n**Priority**: P1-High | **Effort**: 5 hours | **Epic**: 33\n\n**As a** API consumer  \n**I want** API key authentication  \n**So that** only authorized clients can use the conversion API\n\n### Technical Implementation\n- API key validation middleware for `/api/v1/` endpoints\n- Key generation and management endpoints\n- Rate limiting per API key\n- Keys stored in Redis with expiry\n\n### Files to Modify/Create\n- `web/app.py` - Add auth middleware \\(~30 lines\\)\n- `web/auth.py` - API key management \\(new, ~80 lines\\)\n- `tests/unit/test_auth.py` - Auth tests \\(new, ~60 lines\\)\n\n### Verification\n```bash\n# Without API key\ncurl http://localhost:5000/api/v1/jobs\n# Should return: 401 Unauthorized\n\n# With valid API key\ncurl -H \"X-API-Key: my-key\" http://localhost:5000/api/v1/jobs\n# Should return: 200 OK with job list\n\n# With invalid API key\ncurl -H \"X-API-Key: invalid\" http://localhost:5000/api/v1/jobs\n# Should return: 403 Forbidden\n```\n\n### Definition of Done\n- [ ] API key validation middleware in place\n- [ ] 401/403 returned for invalid/missing keys\n- [ ] Rate limiting per API key\n- [ ] Key management endpoints work\n- [ ] All tests pass \\(unit + integration\\)\n- [ ] Documentation updated with auth examples\n\n**References**: Plan lines 1212-1288\nEOF)",
      "Bash(gh issue comment:*)",
      "Bash(gh milestone:*)",
      "Bash(pip3 install:*)",
      "Bash(pip install:*)",
      "Bash(pip3 list:*)",
      "Bash(pipx list:*)",
      "Bash(gh issue close:*)",
      "Bash(git push:*)",
      "Bash(git pull:*)",
      "Bash(git stash:*)",
      "Bash(pip3 show:*)",
      "Bash(echo:*)",
      "Bash(pip show:*)",
      "Bash(sudo journalctl:*)",
      "Bash(docker events:*)"
    ]
  },
  "enableAllProjectMcpServers": true,
  "enabledMcpjsonServers": [
    "context7"
  ]
}
